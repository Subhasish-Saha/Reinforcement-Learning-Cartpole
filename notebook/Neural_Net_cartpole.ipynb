{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = [\n",
    "    tf.keras.layers.Dense(5,activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid') # left prob # 1 < LEFT, O > RIGHT\n",
    "]\n",
    "\n",
    "model = tf.keras.Sequential(LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg_policy(observation, model): # pg = policy gradient\n",
    "    left_probability = model.predict(observation[np.newaxis]) # probability value which lies between 0 and 1\n",
    "    action = int(np.random.rand() > left_probability) # Value will be 0 or 1 # explorations vs exploitation concept\n",
    "    return action "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Policy Gradients\n",
    "\n",
    "Optimize learnable parameters of policy by following the gradients towards higher reward (maximizing reward)\n",
    "\n",
    "## steps\n",
    "\n",
    "    let the NN play the game multiple times and at every step just calculate the gradients (wrt reward) but dont apply it immidiately.\n",
    "    Once you have completed several episodes then compute the actions using discounted method.\n",
    "    result of previous step 2 can +ve or -ve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.6645621]], dtype=float32)>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.uniform([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2])\n",
    "a[np.newaxis]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, observation, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_prabability = model(observation[np.newaxis])\n",
    "        action = (tf.random.uniform([1,1]) > left_prabability) # True and False\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32) # \n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_prabability)) \n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables) # dc/dw\n",
    "    print(int(action))\n",
    "    new_observation, reward, done, info, e = env.step(int(action))\n",
    "    print(new_observation)\n",
    "    print(reward)\n",
    "    print(done)\n",
    "    print(info)\n",
    "    print(e)\n",
    "    return new_observation, reward, done, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = list()\n",
    "    all_grads = list()\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = list()\n",
    "        current_grads = list()\n",
    "        observation = env.reset()\n",
    "        observation = observation[0]\n",
    "        for step in range(n_max_steps):\n",
    "            observation, reward, done, grads = play_one_step(env, observation, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    N = len(rewards)\n",
    "    for step in range(N - 2, -1, -1):\n",
    "        # a_n + a_n+1*gamma\n",
    "        discounted[step] = discounted[step] + discounted[step + 1] * discount_factor\n",
    "    return discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 2])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = np.array([1,2])\n",
    "np.concatenate([x,x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = list()\n",
    "    for reward in all_rewards:\n",
    "        # discounted rewards\n",
    "        drs = discount_rewards(reward, discount_factor)\n",
    "        all_discounted_rewards.append(drs)\n",
    "\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "\n",
    "    normalize_rewards = list()\n",
    "    for discounted_rewards in all_discounted_rewards:\n",
    "        nrs = (discounted_rewards - reward_mean) / reward_std\n",
    "        normalize_rewards.append(nrs)\n",
    "    return normalize_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95\n",
    "learning_rate = 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(seed=SEED)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 0]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = [1,2,3]\n",
    "r2 = [-1,-2,3]\n",
    "all_rewards_1 = [r1, r2]\n",
    "list(map(sum, all_rewards_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(sum, all_rewards_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 3, 4], dtype=int32)>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [[1,2,3], [3,4,5]]\n",
    "tf.reduce_mean(arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [153], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iterations):\n\u001b[0;32m----> 2\u001b[0m     all_rewards, all_grads \u001b[38;5;241m=\u001b[39m \u001b[43mplay_multiple_episodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes_per_update\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_max_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     total_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28msum\u001b[39m, all_rewards))\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_iterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean rewards: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_rewards\u001b[38;5;241m/\u001b[39mn_episodes_per_update\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     )\n",
      "Cell \u001b[0;32mIn [144], line 10\u001b[0m, in \u001b[0;36mplay_multiple_episodes\u001b[0;34m(env, n_episodes, n_max_steps, model, loss_fn)\u001b[0m\n\u001b[1;32m      8\u001b[0m observation \u001b[38;5;241m=\u001b[39m observation[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_max_steps):\n\u001b[0;32m---> 10\u001b[0m     observation, reward, done, grads \u001b[38;5;241m=\u001b[39m \u001b[43mplay_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     current_rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     12\u001b[0m     current_grads\u001b[38;5;241m.\u001b[39mappend(grads)\n",
      "Cell \u001b[0;32mIn [143], line 10\u001b[0m, in \u001b[0;36mplay_one_step\u001b[0;34m(env, observation, model, loss_fn)\u001b[0m\n\u001b[1;32m      8\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables) \u001b[38;5;66;03m# dc/dw\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mint\u001b[39m(action))\n\u001b[0;32m---> 10\u001b[0m new_observation, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mint\u001b[39m(action))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_observation)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(reward)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn\n",
    "    )\n",
    "    total_rewards = sum(map(sum, all_rewards))\n",
    "    print(f\"Iteration: {iteration + 1}/{n_iterations}\",\n",
    "    f\"mean rewards: {total_rewards/n_episodes_per_update}\"\n",
    "    )\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "\n",
    "    all_mean_grads = list()\n",
    "    # Weight of 5 hidden nodes, bias for 5 nodes, w for output node, bias for output node\n",
    "    N = len(model.trainable_variables)\n",
    "    for var_index in range(N):\n",
    "        temp_reduce_mean = list()\n",
    "        for episode_index, final_rewards in enumerate(all_final_rewards): # rewards for every episode\n",
    "            for step, final_reward in enumerate(final_rewards): # several steps\n",
    "                result = final_reward * all_grads[episode_index][step][var_index]\n",
    "                temp_reduce_mean.append(result)\n",
    "        mean_grads = tf.reduce_mean(temp_reduce_mean, axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "import time\n",
    "\n",
    "unique_name = re.sub(r\"[\\s+:]\", \"_\", time.asctime())\n",
    "model_name = f\"model_at_{unique_name}_.h5\"\n",
    "model.save(model_name)\n",
    "print(f\"model is saved as '{model_name}'\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b13d25ea61d4bef501afd714eb6df7a784ea42b56f95d0b6f56b01abc02d663e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
